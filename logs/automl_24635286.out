==========================================================
Job Started on n-62-30-23
Job ID: 24635286
Working Directory: /zhome/44/a/187127/school/Water-level-forecasting-new-project
Requested Cores: 8
Allocated Hosts: n-62-30-23 n-62-30-23 n-62-30-23 n-62-30-23 n-62-30-23 n-62-30-23 n-62-30-23 n-62-30-23
Queue: hpc
Start Time: Thu Apr 10 17:50:43 CEST 2025
==========================================================
Loading required modules...
Modules loaded:
Activating Conda environment 'forecasting'...
Conda environment: forecastinghpc
Python path: /zhome/44/a/187127/anaconda3/envs/forecastinghpc/bin/python
Working directory set to: /zhome/44/a/187127/school/Water-level-forecasting-new-project
Running automl.py...

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 24635286: <AutoML_WaterLevel> in cluster <dcc> Exited

Job <AutoML_WaterLevel> was submitted from host <hpclogin1> by user <s224296> in cluster <dcc> at Thu Apr 10 17:50:39 2025
Job was executed on host(s) <8*n-62-30-23>, in queue <hpc>, as user <s224296> in cluster <dcc> at Thu Apr 10 17:50:39 2025
</zhome/44/a/187127> was used as the home directory.
</zhome/44/a/187127/school/Water-level-forecasting-new-project> was used as the working directory.
Started at Thu Apr 10 17:50:39 2025
Terminated at Thu Apr 10 17:54:29 2025
Results reported at Thu Apr 10 17:54:29 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
# LSF Batch Job Script for running automl.py

### General LSF options ###

# -- Specify the queue --
# Use a GPU queue appropriate for your models (e.g., gpuv100 or gpua100)
# Remember A100 requires code compiled with CUDA >= 11.0
#BSUB -q hpc

# -- Set the job Name --
#BSUB -J AutoML_WaterLevel

# -- Ask for number of cores (CPU slots) --
# Adjust based on data loading/preprocessing needs. 8 is a reasonable start.
#BSUB -n 8

# -- Request GPU resources --
# Request 1 GPU in exclusive process mode.
#llBSUB -gpu "num=1:mode=exclusive_process"ll

# -- Specify that all cores/GPU must be on the same host/node --
#BSUB -R "span[hosts=1]"

# -- Specify memory requested PER CORE/SLOT --
# Example: 8GB RAM per core (total 64GB). ADJUST BASED ON YOUR NEEDS!
#BSUB -R "rusage[mem=8GB]"

# -- Specify memory limit PER CORE/SLOT (Job killed if exceeded) --
# Example: 10GB per core (total 80GB limit). ADJUST BASED ON YOUR NEEDS!
#BSUB -M 9GB

# -- Set walltime limit: hh:mm --
# Max 24:00 for GPU queues. START SHORT (e.g., 1:00) FOR TESTING!
# Adjust based on expected runtime for the full job.
#BSUB -W 01:00

# -- Specify output and error files (%J expands to Job ID) --
# We'll create the 'logs' directory below.
#BSUB -o logs/automl_%J.out
#BSUB -e logs/automl_%J.err

# -- Email notifications (Optional) --
# Uncomment and set your DTU email if desired
##BSUB -u s224296@dtu.dk  # Use your actual email
# Send email on job start (-B) and job end/failure (-N)
##BSUB -B
##BSUB -N

### End of LSF options ###

(... more ...)
------------------------------------------------------------

Exited with exit code 143.

Resource usage summary:

    CPU time :                                   362.00 sec.
    Max Memory :                                 498 MB
    Average Memory :                             408.00 MB
    Total Requested Memory :                     65536.00 MB
    Delta Memory :                               65038.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                52
    Run time :                                   230 sec.
    Turnaround time :                            230 sec.

The output (if any) is above this job summary.



PS:

Read file <logs/automl_24635286.err> for stderr output of this job.

